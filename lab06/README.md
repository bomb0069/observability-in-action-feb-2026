# Lab06 - Auto-Setup Kibana Index Patterns for Multi-Application Logs

Lab à¸ªà¸³à¸«à¸£à¸±à¸šà¸—à¸”à¸ªà¸­à¸šà¸à¸²à¸£à¹€à¸à¹‡à¸š logs à¸ˆà¸²à¸ multiple applications à¸—à¸µà¹ˆà¸¡à¸µ format à¸•à¹ˆà¸²à¸‡à¸à¸±à¸™ à¸žà¸£à¹‰à¸­à¸¡ **automatic Kibana index pattern creation** à¹€à¸¡à¸·à¹ˆà¸­à¸£à¸±à¸™ docker-compose

## Key Features

âœ¨ **Automatic Kibana Setup**

- Index patterns à¸–à¸¹à¸à¸ªà¸£à¹‰à¸²à¸‡à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´à¹€à¸¡à¸·à¹ˆà¸­ start services
- à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ manual configuration à¹ƒà¸™ Kibana UI
- Dashboard à¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸—à¸±à¸™à¸—à¸µà¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸ start

## Architecture

```
Flog (Apache Combined Format)
  â†’ access.log (1 log/sec)
    â†“
Flog2 (JSON Format)                  â†’ Filebeat (reads both files)
  â†’ app.log (2 logs/sec)                â†’ Logstash (processes different formats)
                                          â†’ Elasticsearch (stores in separate indices)
                                            â†’ Kibana (visualizes both sources)
```

## Components

- **Flog**: Fake log generator à¸—à¸µà¹ˆà¸ªà¸£à¹‰à¸²à¸‡ Apache combined format logs (1 log/second)
- **Flog2**: Fake log generator à¸—à¸µà¹ˆà¸ªà¸£à¹‰à¸²à¸‡ JSON format logs (2 logs/second) - à¸¡à¸µà¸›à¸£à¸´à¸¡à¸²à¸“à¸¡à¸²à¸à¸à¸§à¹ˆà¸² flog 2 à¹€à¸—à¹ˆà¸²
- **Filebeat**: Log shipper à¸—à¸µà¹ˆà¸­à¹ˆà¸²à¸™ log files à¸ˆà¸²à¸ 2 sources à¹à¸¥à¸°à¸ªà¹ˆà¸‡à¹„à¸›à¸¢à¸±à¸‡ Logstash
- **Logstash**: Log processor à¸—à¸µà¹ˆ parse logs à¸•à¸²à¸¡ format à¸—à¸µà¹ˆà¸•à¹ˆà¸²à¸‡à¸à¸±à¸™
  - Apache logs: à¹ƒà¸Šà¹‰ grok patterns à¸žà¸£à¹‰à¸­à¸¡ GeoIP à¹à¸¥à¸° User Agent parsing
  - JSON logs: parse à¹‚à¸”à¸¢ Filebeat à¹à¸¥à¹‰à¸§à¸ªà¹ˆà¸‡à¸•à¹ˆà¸­à¹„à¸›à¸¢à¸±à¸‡ Elasticsearch
- **Elasticsearch**: Search and analytics engine à¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸à¹‡à¸š logs à¹ƒà¸™ 2 indices:
  - `flog-logs-*`: Apache combined format logs
  - `flog2-logs-*`: JSON format logs
- **Kibana**: Visualization platform à¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¸£à¹‰à¸²à¸‡ charts, graphs à¹à¸¥à¸° dashboards

## Lab Objectives

1. à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸à¸²à¸£ collect logs à¸ˆà¸²à¸ multiple applications à¸žà¸£à¹‰à¸­à¸¡à¸à¸±à¸™
2. à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸à¸²à¸£à¹à¸¢à¸ log formats à¸—à¸µà¹ˆà¹à¸•à¸à¸•à¹ˆà¸²à¸‡à¸à¸±à¸™ (Apache vs JSON)
3. à¹ƒà¸Šà¹‰ Filebeat à¹ƒà¸™à¸à¸²à¸£à¸­à¹ˆà¸²à¸™ multiple log files à¹à¸¥à¸° tag à¹à¸•à¹ˆà¸¥à¸° source
4. à¹ƒà¸Šà¹‰ Logstash à¹ƒà¸™à¸à¸²à¸£ process logs à¹à¸¢à¸à¸•à¸²à¸¡ format
5. à¸ˆà¸±à¸”à¹€à¸à¹‡à¸š logs à¹ƒà¸™ separate Elasticsearch indices à¸•à¸²à¸¡ application
6. **Automatic Kibana index pattern creation** - à¸ªà¸£à¹‰à¸²à¸‡à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´à¸œà¹ˆà¸²à¸™ API
7. à¸ªà¸£à¹‰à¸²à¸‡ visualizations à¹à¸¥à¸° dashboards à¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š logs à¸ˆà¸²à¸ 2 sources
8. à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸›à¸£à¸´à¸¡à¸²à¸“ logs à¸—à¸µà¹ˆà¹à¸•à¸à¸•à¹ˆà¸²à¸‡à¸à¸±à¸™ (flog2 à¸¡à¸µà¸›à¸£à¸´à¸¡à¸²à¸“à¸¡à¸²à¸à¸à¸§à¹ˆà¸² flog 2 à¹€à¸—à¹ˆà¸²)

## Prerequisites

- Docker
- Docker Compose

## Quick Start

1. Start all services:

```bash
docker-compose up -d
```

2. Check services status:

```bash
docker-compose ps
```

3. View logs to monitor data flow:

```bash
# Watch flog generating Apache logs (1 log/sec)
docker-compose logs -f flog

# Watch flog2 generating JSON logs (2 logs/sec)
docker-compose logs -f flog2

# Watch filebeat shipping logs from both sources
docker-compose logs -f filebeat

# Watch logstash processing logs
docker-compose logs -f logstash
```

4. Access Kibana:

- URL: http://localhost:5601
- Wait 2-3 minutes for automatic setup to complete
- **Index patterns are created automatically** - no manual configuration needed!
- Dashboard is ready to use immediately

5. Monitor setup progress:

```bash
# Watch the automatic setup process
docker-compose logs -f kibana-setup
```

You should see:

```
âœ“ Data found in both indices
âœ“ Index pattern created: flog-logs-*
âœ“ Index pattern created: flog2-logs-*
âœ“ Successfully imported saved objects!
Setup complete!
```

## Automatic Setup Details

The `kibana-setup` service automatically performs:

1. **Waits for Kibana** to be fully ready
2. **Waits for data** in Elasticsearch indices (both flog-logs and flog2-logs)
3. **Creates index patterns** via Kibana API:
   - `flog-logs-*` for Apache logs
   - `flog2-logs-*` for JSON logs
4. **Sets default index pattern** to `flog-logs-*`
5. **Imports dashboard** with visualizations

All of this happens automatically when you run `docker-compose up -d`! ðŸŽ‰

## Verify Data Collection

### Check Elasticsearch Indices

```bash
# Check flog indices (Apache logs)
curl -s "http://localhost:9200/_cat/indices/flog-logs-*?v"

# Check flog2 indices (JSON logs)
curl -s "http://localhost:9200/_cat/indices/flog2-logs-*?v"

# Count documents in each index
curl -s "http://localhost:9200/flog-logs-*/_count" | jq
curl -s "http://localhost:9200/flog2-logs-*/_count" | jq
```

### Expected Results

- **flog-logs-\***: ~60 documents per minute (1 log/second)
- **flog2-logs-\***: ~120 documents per minute (2 logs/second)

After 5 minutes:

- flog-logs: ~300 documents
- flog2-logs: ~600 documents (2x more than flog)

### Sample Data Comparison

Apache log (flog):

```
244.165.230.81 - - [05/Feb/2026:10:15:30 +0000] "GET /api/users HTTP/1.1" 200 1234 "-" "Mozilla/5.0..."
```

JSON log (flog2):

```json
{
  "host": "244.165.230.81",
  "user-identifier": "-",
  "datetime": "05/Feb/2026:10:15:30 +0000",
  "method": "GET",
  "request": "/api/products",
  "protocol": "HTTP/1.1",
  "status": 200,
  "size": 5678,
  "referer": "-",
  "user-agent": "Mozilla/5.0..."
}
```

## Using Kibana

### Index Patterns (Auto-created!)

Index patterns are **automatically created** when you start the services! No manual steps needed.

To verify:

1. Open Kibana at http://localhost:5601
2. Go to: **Management â†’ Stack Management â†’ Index Patterns**
3. You should see:
   - âœ… `flog-logs-*` (Apache logs) - set as default
   - âœ… `flog2-logs-*` (JSON logs)

### Pre-built Dashboard (Auto-imported)

Dashboard is also automatically imported and ready to use:

1. Go to: **Analytics â†’ Dashboard**
2. Open **"Apache Logs Overview"** dashboard

Dashboard includes 7 visualizations for Apache logs (flog):

- HTTP Status Codes (Pie chart)
- Top 10 IPs (Bar chart)
- Traffic Over Time (Line chart)
- Top URLs (Table)
- Browser Distribution (Donut chart)
- Average Response Size (Area chart)
- HTTP Methods (Horizontal bar)

### Explore Both Data Sources

**Explore flog (Apache logs):**

1. Go to: **Analytics â†’ Discover**
2. Select `flog-logs-*` index pattern
3. Available fields:
   - `http.response.status_code`: HTTP status codes
   - `source.address`: Client IP addresses
   - `http.request.method`: HTTP methods (GET, POST, etc.)
   - `url.original`: Request URLs
   - `http.response.body.bytes`: Response sizes
   - `user_agent.original`: User agent strings
   - `geoip.*`: Geographic information

**Explore flog2 (JSON logs):**

1. Go to: **Analytics â†’ Discover**
2. Select `flog2-logs-*` index pattern
3. Available fields (already parsed by Filebeat):
   - `host`: Source IP
   - `method`: HTTP method
   - `request`: Request path
   - `protocol`: HTTP protocol version
   - `status`: HTTP status code
   - `size`: Response size
   - `user-agent`: User agent string
   - `log_type`: "json"
   - `app`: "flog2"

### Create Additional Visualizations

Since index patterns are auto-created, you can immediately start creating custom visualizations:

**Compare Log Volume:**

1. Go to: **Analytics â†’ Visualize Library â†’ Create visualization**
2. Choose **"Line"** chart
3. Add data series:
   - **Series 1**: Select `flog-logs-*`, Metric: Count, Bucket: Date Histogram on `@timestamp`
   - **Series 2**: Select `flog2-logs-*`, Metric: Count, Bucket: Date Histogram on `@timestamp`
4. Title: "Log Volume Comparison (flog vs flog2)"
5. Save visualization

Expected result: flog2 line should show ~2x volume compared to flog

**Compare HTTP Status Distribution:**

1. Create two separate Pie charts:
   - **Chart 1**: Index pattern `flog-logs-*`, Bucket: Terms on `http.response.status_code`
   - **Chart 2**: Index pattern `flog2-logs-*`, Bucket: Terms on `status`
2. Add both to a new dashboard for side-by-side comparison

### Create Multi-Source Dashboard

With auto-created index patterns, you can immediately build dashboards:

1. Go to: **Analytics â†’ Dashboard â†’ Create dashboard**
2. Title: "Multi-App Log Monitoring"
3. Add visualizations:
   - Log Volume Comparison (both sources)
   - HTTP Status Codes - flog (Apache logs)
   - HTTP Status Codes - flog2 (JSON logs)
   - Traffic timeline for each source
4. Set auto-refresh: 10 seconds
5. Save dashboard

## Understanding the Data

### Apache Logs (flog) - Structured Format

**Raw log:**

```
244.165.230.81 - - [05/Feb/2026:10:15:30 +0000] "GET /api/users HTTP/1.1" 200 1234 "-" "Mozilla/5.0..."
```

**After Logstash processing:**

- Parsed with grok pattern
- Enhanced with GeoIP data (location, country, city)
- User agent parsed (browser, OS, device)
- Stored in ECS format: `http.response.status_code`, `source.address`, etc.

### JSON Logs (flog2) - Semi-Structured Format

**Raw log:**

```json
{
  "host": "244.165.230.81",
  "user-identifier": "-",
  "datetime": "05/Feb/2026:10:15:30 +0000",
  "method": "GET",
  "request": "/api/products",
  "protocol": "HTTP/1.1",
  "status": 200,
  "size": 5678,
  "referer": "-",
  "user-agent": "Mozilla/5.0..."
}
```

**After Filebeat processing:**

- Already in JSON format, parsed automatically
- Fields available as-is: `host`, `method`, `request`, `status`, `size`
- No additional enrichment (no GeoIP, no user agent parsing)
- Tagged with `app: flog2` and `log_type: json`

## Key Differences Between Two Sources

| Aspect          | flog (Apache)                 | flog2 (JSON)                                |
| --------------- | ----------------------------- | ------------------------------------------- |
| **Format**      | Apache Combined Log           | JSON                                        |
| **Log Rate**    | 1 log/second                  | 2 logs/second                               |
| **File**        | `/logs/access.log`            | `/logs/app.log`                             |
| **Processing**  | Logstash grok parsing         | Filebeat JSON parsing                       |
| **Enrichment**  | GeoIP + User Agent            | None                                        |
| **Index**       | `flog-logs-*`                 | `flog2-logs-*`                              |
| **Field Names** | ECS format (http._, source._) | Original JSON fields (host, method, status) |

## Verify Data Flow

### Check Both Applications Are Generating Logs

```bash
# Check log files
ls -lh logs/

# Watch Apache logs (flog)
tail -f logs/access.log

# Watch JSON logs (flog2)
tail -f logs/app.log
```

### Check Elasticsearch Indices

```bash
# List all indices
curl http://localhost:9200/_cat/indices?v

# Count documents in each index
curl -s "http://localhost:9200/flog-logs-*/_count" | jq
curl -s "http://localhost:9200/flog2-logs-*/_count" | jq
```

### Query Sample Documents

```bash
# Get sample from flog (Apache)
curl -s "http://localhost:9200/flog-logs-*/_search?size=1&pretty"

# Get sample from flog2 (JSON)
curl -s "http://localhost:9200/flog2-logs-*/_search?size=1&pretty"
```

## Advanced Usage

### Monitor Log Rate Difference

After running for 5 minutes:

```bash
# Get counts
FLOG_COUNT=$(curl -s "http://localhost:9200/flog-logs-*/_count" | jq .count)
FLOG2_COUNT=$(curl -s "http://localhost:9200/flog2-logs-*/_count" | jq .count)

echo "flog logs: $FLOG_COUNT"
echo "flog2 logs: $FLOG2_COUNT"
echo "flog2/flog ratio: $(echo "scale=2; $FLOG2_COUNT / $FLOG_COUNT" | bc)"
```

Expected ratio: ~2.0 (flog2 produces 2x more logs)

## Useful Kibana Query Language (KQL) Examples

### For flog (Apache logs) - using ECS fields:

```
# Find all errors (4xx and 5xx)
http.response.status_code >= 400

# Specific HTTP method
http.request.method: "GET"

# Exclude certain IPs
NOT source.address: "192.168.1.1"

# Large responses
http.response.body.bytes > 50000

# Combine conditions
http.response.status_code >= 500 AND geoip.country_name: "Thailand"
```

### For flog2 (JSON logs) - using original field names:

```
# Find all errors
status >= 400

# Specific HTTP method
method: "GET"

# Exclude certain hosts
NOT host: "192.168.1.1"

# Large responses
size > 50000

# Combine conditions
status >= 500 AND app: "flog2"
```

## Troubleshooting

### Dashboard not showing up

1. Check if kibana-setup completed successfully:

```bash
docker-compose logs kibana-setup
```

2. If import failed, retry manually:

```bash
docker-compose restart kibana-setup
```

3. Or import manually via Kibana UI:
   - Go to **Management â†’ Stack Management â†’ Saved Objects**
   - Click **"Import"**
   - Select `kibana/saved_objects.ndjson`

### No visualizations showing data

1. Check time range in top-right corner (default is last 15 minutes)
2. Verify both applications are generating logs:

```bash
docker-compose logs flog | tail
docker-compose logs flog2 | tail
ls -lh logs/
tail -f logs/access.log
tail -f logs/app.log
```

3. Wait a few minutes for logs to flow through the pipeline
4. Check data exists in Discover for both index patterns

### flog2 logs not appearing

1. Check if flog2 container is running:

```bash
docker-compose ps flog2
```

2. Verify JSON logs are being created:

```bash
cat logs/app.log
```

3. Check Filebeat is reading both files:

```bash
docker-compose logs filebeat | grep -E "(access.log|app.log)"
```

4. Verify data in Elasticsearch:

```bash
curl -s "http://localhost:9200/flog2-logs-*/_count" | jq
```

### Field not available for visualization

1. Go to **Management â†’ Index Patterns â†’ flog-logs-\***
2. Click **"Refresh field list"**
3. Verify the field exists in the list
4. Check if Logstash is parsing logs correctly:

```bash
docker-compose logs logstash | grep -i error
```

### Services not starting properly

1. Check all services are running:

```bash
docker-compose ps
```

2. View logs for specific service:

```bash
docker-compose logs elasticsearch
docker-compose logs kibana
docker-compose logs logstash
```

3. Restart services if needed:

```bash
docker-compose restart
```

## Exporting and Backing Up

### Export Dashboard

1. Go to **Management â†’ Stack Management â†’ Saved Objects**
2. Select objects to export (dashboard + visualizations)
3. Click **"Export"**
4. Save the NDJSON file as backup

### Re-import Dashboard

1. Go to **Management â†’ Stack Management â†’ Saved Objects**
2. Click **"Import"**
3. Select your NDJSON file
4. Handle conflicts (overwrite or skip)

## Learning Resources

### Kibana Documentation

- [Kibana Guide](https://www.elastic.co/guide/en/kibana/current/index.html)
- [Lens](https://www.elastic.co/guide/en/kibana/current/lens.html)
- [Dashboard](https://www.elastic.co/guide/en/kibana/current/dashboard.html)

### Visualization Best Practices

- Use appropriate chart types for your data
- Keep dashboards focused and uncluttered
- Use consistent colors across visualizations
- Add filters for interactivity
- Set meaningful titles and labels

## Stop Services

```bash
# Stop services
docker-compose stop

# Stop and remove containers
docker-compose down

# Stop and remove everything including volumes
docker-compose down -v
```

## Ports Used

- 5601: Kibana
- 9200: Elasticsearch HTTP
- 9300: Elasticsearch TCP
- 5044: Logstash Beats input
- 9600: Logstash monitoring

## Summary

This lab demonstrates:

- âœ… Setting up complete ELK stack
- âœ… Parsing Apache combined logs with Logstash
- âœ… Creating various visualization types in Kibana
- âœ… Building interactive dashboards
- âœ… Real-time log monitoring
- âœ… Geographic analysis with GeoIP
- âœ… User agent analysis
- âœ… Traffic pattern analysis

## Next Steps

1. Export dashboard and visualizations
2. Create alerts based on log patterns
3. Implement log retention policies
4. Add more data sources
5. Create custom Logstash filters
6. Explore Machine Learning features in Kibana
